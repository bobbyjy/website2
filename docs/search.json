[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Corey Cassell",
    "section": "",
    "text": "A Portland Oregon based engineer and masters in data science candidate with 7+ years of broad-based experience utilizing advanced data and analytical tools, building production grade metrics and visualizations, while integrating industrial engineering principles to drive continuous improvement for domestic and international customers."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Corey Cassell",
    "section": "Education",
    "text": "Education\nWillamette University\nMasters in Data Science (MSDS) Planned Graduation August 2023\nCalifornia Polytechnical State University, San Luis Obispo\nBachelor of Science, Industrial Engineering"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Corey Cassell",
    "section": "Skills",
    "text": "Skills\nSOFTWARE, PLATFORMS, AND PROGRAMMING\n\nSQL – Accessing, cleaning, and joining high volume data in a fast-paced manufacturing environment.\nR – Crafting visualizations through base\nR and ggplot. Writing web scraping scripts. Tuning and implementing machine learning models.\nTableau – Design, build, implementation of production facing metrics and analytical tools for manager and executive level communication. Improving user access and understanding of production performance to enable better allocation of resources.\nAccess/Excel/VBA – creating automated metrics and scrips to answer emergent questions.\nBASH – Basic file navigation and manipulation. Remote server interaction. Docker file building and deployment.\n\nGitHub – Project management and version control.\nPython – Basic level understanding and capability.\n\nDATA SCIENCE\n\nData Visualization – Using R and Tableau to analyze and interpret business anomalies through uncovering hidden trends.\nData Engineering – Employing the ETL process in the setup, design, and management of PostgreSQL server/database relationship. joining of tables from cross platform databases (Teradata/Oracle/Microsoft SQL Server) and transforming information into useable content.\n\nMachine learning – Leveraging linear regression-based models to predict next day manufacturing performance for more efficient current day resource allocation.\nSurvival Analysis and survival curve plotting to predict product life expectancy"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Corey Cassell",
    "section": "Experience",
    "text": "Experience\nINDUSTRIAL ENGINEER III, BOEING October 2016 – Current\nBOEING PORTLAND, OR, USA – SUPPORT FOR ACTUATION SYSTEMS MANUFACTURING TEAM\n\nTeam focal for analytics and metrics development using Tableau, Access, and Excel platforms\nGeneration of Tableau dashboards through the integration of multiplatform SQL query building\nDevelopment of statistical standard processes for production performance visibility including data cleaning, preparation, analysis, and site leadership level communication\nAutomated daily report design, construction, and support\nMachining cell layout design and implementation\nDetailed production schedule recovery planning\nPlant staffing and machining capacity analysis in dynamic demand environment\n\nBOEING SHEFFIELD, UK – 7 MONTHS TEMPORARY ASSIGNMENT\n\nUser story design and development for web metric deployment\nOperation level time study analysis enabling targeted problem solving for cycle time reduction\nTraining and support of Sheffield IE team\nLean 3P (production preparation process) workshop support for transmission housings\nThird party supplier capacity and supply chain buffering analysis\nWork statement and movement scenario analysis providing solutions for over capacity cells\nNew product cross site production planning and integration"
  },
  {
    "objectID": "InteractiePlots.html",
    "href": "InteractiePlots.html",
    "title": "Interactive Plots",
    "section": "",
    "text": "Show the code\nds2 <- read_rds(\"https://raw.githubusercontent.com/bobbyjy/MyData/main/pinot_orig.rds\")\ndatatable(head(ds2))"
  },
  {
    "objectID": "InteractiePlots.html#plot-1-using-ggiraph",
    "href": "InteractiePlots.html#plot-1-using-ggiraph",
    "title": "Interactive Plots",
    "section": "Plot 1 (Using ggiraph)",
    "text": "Plot 1 (Using ggiraph)\n\n\nShow the code\ngg <- ds2 %>% \n  select(taster_name) %>% \n  group_by(taster_name) %>% \n  summarise(taster_name, reviews = n()) %>% \n  distinct() %>% \n  arrange(reviews) %>% \n  ggplot(aes(reviews,reorder(taster_name,reviews), fill = taster_name))+\n  #geom_col()+\n  geom_col_interactive(aes(tooltip = taster_name, data_id = taster_name), show.legend = FALSE)+\n  scale_fill_viridis_d()+\n  labs(title = \"Which tasters have tasted the most?\")\nx <- girafe(ggobj = gg, width_svg = 8, height_svg = 6,\n            options = list(\n              opts_hover_inv(css = \"opacity:0.1;\"),\n              opts_hover(css= \"stroke-width:2;\")\n            ))\nx"
  },
  {
    "objectID": "InteractiePlots.html#plot-2-using-plotly",
    "href": "InteractiePlots.html#plot-2-using-plotly",
    "title": "Interactive Plots",
    "section": "Plot 2 (Using Plotly)",
    "text": "Plot 2 (Using Plotly)\n\n\nShow the code\na<-ds2 %>% \n  ggplot(aes(price,points, text = paste(\"tasted by: \", taster_name,\n                                        \"<br>\",\n                                         \"description: \", description), color = points))+\n  geom_point(show.legend = FALSE)+\n  theme_bw()+\n  labs(title = \"Wine Price vs. Rating\", x = \"Rating\", y = \"Price\")\nggp <- ggplotly(a)\nggp %>% layout(hovermode = \"closest\")"
  },
  {
    "objectID": "Presentation.html#about-me",
    "href": "Presentation.html#about-me",
    "title": "First RevealJs Deck!",
    "section": "About Me",
    "text": "About Me\n\nMy name is Corey Cassell, I am an engineer in the commercial aerospace industry and an aspiring data scientist.\nI am currently closing out the final half of my data science masters program.\nIn particular I am interested in applying machine learning models to manufacturing problems to help predict and respond to daily production gaps before they occur. ## Here’s a classification project example that I completed for school\nI will be using the caret library in conjunction with the wine dataset to do this analysis\nThe data I am using contains wines from different regions with price, rating, and description information.\nThe target variable is the wine province of origin.\nLets take a look at the dataset!"
  },
  {
    "objectID": "Presentation.html#sources-and-data",
    "href": "Presentation.html#sources-and-data",
    "title": "First RevealJs Deck!",
    "section": "Sources and Data",
    "text": "Sources and Data"
  },
  {
    "objectID": "Presentation.html#text-parsing",
    "href": "Presentation.html#text-parsing",
    "title": "First RevealJs Deck!",
    "section": "Text Parsing",
    "text": "Text Parsing\n\nThe following function was used to take in a dataset with a text field, remove any stop words, only include words that occured over a certain threshold, and determine sensitivity to word roots."
  },
  {
    "objectID": "Presentation.html#feature-engineering",
    "href": "Presentation.html#feature-engineering",
    "title": "First RevealJs Deck!",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nIn this step I apply the previously discussed function and set my word thrshold at 1000 occurences.\nExploring the data I observed that of the 6 provinces I was attempting to predict, 4 were extremely under represented.\nTo address this I chose to resample and even out the lower represented provinces while undersampling the higher representation provinces."
  },
  {
    "objectID": "Presentation.html#results",
    "href": "Presentation.html#results",
    "title": "First RevealJs Deck!",
    "section": "Results",
    "text": "Results\n\nTo understand the overall validity of my model I used Kappa (the measure of how likely it would be to achieve these results at random) and a confusion matrix to visualize the quantity of correct and incorrect predictions by category.\n\n\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               198         15                 0           3        1\n  California              11        613                 5           5        9\n  Casablanca_Valley        0         12                21           3        1\n  Marlborough              1         23                 0          24        1\n  New_York                 1         21                 0           0       10\n  Oregon                  27        107                 0          10        4\n                   Reference\nPrediction          Oregon\n  Burgundy              24\n  California            90\n  Casablanca_Valley     14\n  Marlborough           25\n  New_York              10\n  Oregon               384\n\nOverall Statistics\n                                          \n               Accuracy : 0.7472          \n                 95% CI : (0.7256, 0.7678)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.6206          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.8319            0.7750                  0.80769\nSpecificity                   0.9700            0.8639                  0.98179\nPos Pred Value                0.8216            0.8363                  0.41176\nNeg Pred Value                0.9721            0.8106                  0.99692\nPrevalence                    0.1423            0.4728                  0.01554\nDetection Rate                0.1184            0.3664                  0.01255\nDetection Prevalence          0.1441            0.4381                  0.03048\nBalanced Accuracy             0.9010            0.8195                  0.89474\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                     0.53333        0.384615        0.7020\nSpecificity                     0.96929        0.980571        0.8686\nPos Pred Value                  0.32432        0.238095        0.7218\nNeg Pred Value                  0.98687        0.990190        0.8571\nPrevalence                      0.02690        0.015541        0.3270\nDetection Rate                  0.01435        0.005977        0.2295\nDetection Prevalence            0.04423        0.025105        0.3180\nBalanced Accuracy               0.75131        0.682593        0.7853"
  },
  {
    "objectID": "Presentation2.html#about-me",
    "href": "Presentation2.html#about-me",
    "title": "First RevealJs Deck!",
    "section": "About Me",
    "text": "About Me\n\nMy name is Corey Cassell, I am an engineer in the commercial aerospace industry and an aspiring data scientist.\nI am currently closing out the final half of my data science masters program.\nIn particular I am interested in applying machine learning models to manufacturing problems to help predict and respond to daily production gaps before they occur.\n\n\n\n\n\n\n\n## Here’s a classification project example that I completed for school - I will be using the caret library in conjunction with the wine dataset to do this analysis - The data I am using contains wines from different regions with price, rating, and description information. - The target variable is the wine province of origin. - Lets take a look at the dataset!"
  },
  {
    "objectID": "Presentation2.html#sources-and-data",
    "href": "Presentation2.html#sources-and-data",
    "title": "First RevealJs Deck!",
    "section": "Sources and Data",
    "text": "Sources and Data"
  },
  {
    "objectID": "Presentation2.html#text-parsing",
    "href": "Presentation2.html#text-parsing",
    "title": "First RevealJs Deck!",
    "section": "Text Parsing",
    "text": "Text Parsing\n\nThe following function was used to take in a dataset with a text field, remove any stop words, only include words that occured over a certain threshold, and determine sensitivity to word roots. ::: {.cell}\n\n\nCode\nwine_words <- function(df, j = 1000, stem=F){ \n  library(tidytext)\n  library(SnowballC)\n  data(stop_words)\n\n  words <- df %>%\n    unnest_tokens(word, description) %>%\n    anti_join(stop_words) %>% # get rid of stop words\n    filter(!(word %in% c(\"wine\",\"pinot\",\"vineyard\")))\n  \n  if(stem){\n    words <- words %>% \n      mutate(word = wordStem(word))\n  }\n  \n  words <- words %>% \n    count(id, word) %>% \n    group_by(id) %>% \n    mutate(exists = (n>0)) %>% \n    ungroup %>% \n    group_by(word) %>% \n    mutate(total = sum(n)) %>% \n    filter(total > j) %>% \n    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% \n    right_join(dplyr::select(df,id,province)) %>% \n    dplyr::select(-id) %>% \n    mutate(across(-province, ~replace_na(.x, F)))\n}\n\n:::"
  },
  {
    "objectID": "Presentation2.html#feature-engineering",
    "href": "Presentation2.html#feature-engineering",
    "title": "First RevealJs Deck!",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nIn this step I apply the previously discussed function and set my word thrshold at 1000 occurences.\nExploring the data I observed that of the 6 provinces I was attempting to predict, 4 were extremely under represented.\nTo address this I chose to resample and even out the lower represented provinces while undersampling the higher representation provinces. ::: {.cell}\n\n\nCode\nset.seed(500)\nwine = read_rds(\"https://raw.githubusercontent.com/bobbyjy/MyData/main/pinot.rds\")\n#training the fit on the adjusted data set, played with some weightings but this didn't help a lot\nwine <- wine %>% \n  mutate(points = scale(points, center = T, scale = T)) %>%\n  mutate(price = scale(log(price), center = T, scale = T)) \n\nwino <- wine_words(wine, j=1000, stem = T)\nwino$price <- wine$price\nwino$points <- wine$points\nwine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)\ntrain <- wino[ wine_index, ]\n###the model can't look at the test set...###\ntest <- wino[-wine_index, ]\n\n####conduct data duplication on only the train set####\nsample_size = 400\n\nburg_sample <- train %>%\n  filter(province=='Burgundy') %>% \n    sample_n(sample_size, replace = T)\n\ncali_sample <- train %>%\n  filter(province=='California') %>% \n    sample_n(800, replace = T) #800 got .84\n\ncasa_sample <- train %>%\n  filter(province=='Casablanca_Valley') %>% \n    sample_n(sample_size, replace = T)\n\nmarl_sample <- train %>%\n  filter(province=='Marlborough') %>% \n    sample_n(sample_size, replace = T)\n\nyork_sample <- train %>%\n  filter(province=='New_York') %>% \n    sample_n(sample_size, replace = T)\n\noreg_sample <- train %>%\n  filter(province=='Oregon') %>% \n    sample_n(800, replace = T) #800 got .84\n\ntrain <- rbind(burg_sample,cali_sample,casa_sample,marl_sample,york_sample,oreg_sample)\n\n#train\n\nweight_train <- train %>% \n  mutate(weights=case_when(\n    province==\"Burgundy\" ~ 1,\n    province==\"California\" ~ 1,\n    province==\"Casablanca_Valley\" ~ 1,\n    province==\"Marlborough\" ~ 1,\n    province==\"New_York\" ~ 1,\n    province==\"Oregon\" ~ 1))\n\nctrl <- trainControl(method = \"repeatedcv\", number = 5, repeats = 1)\n\nfit <- train(province ~ .,\n             data = train,\n             method = \"rf\",\n             trControl = ctrl,\n             ntree=100,\n             weights = weight_train$weights\n             )\n\n pred <- predict(fit, newdata=test)\n\n:::"
  },
  {
    "objectID": "Presentation2.html#results",
    "href": "Presentation2.html#results",
    "title": "First RevealJs Deck!",
    "section": "Results",
    "text": "Results\n\nTo understand the overall validity of my model I used Kappa (the measure of how likely it would be to achieve these results at random) and a confusion matrix to visualize the quantity of correct and incorrect predictions by category. ::: {.cell}\n\n\nCode\n confusionMatrix(factor(pred),factor(test$province))\n\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               198         15                 0           3        1\n  California              11        613                 5           5        9\n  Casablanca_Valley        0         12                21           3        1\n  Marlborough              1         23                 0          24        1\n  New_York                 1         21                 0           0       10\n  Oregon                  27        107                 0          10        4\n                   Reference\nPrediction          Oregon\n  Burgundy              24\n  California            90\n  Casablanca_Valley     14\n  Marlborough           25\n  New_York              10\n  Oregon               384\n\nOverall Statistics\n                                          \n               Accuracy : 0.7472          \n                 95% CI : (0.7256, 0.7678)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.6206          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.8319            0.7750                  0.80769\nSpecificity                   0.9700            0.8639                  0.98179\nPos Pred Value                0.8216            0.8363                  0.41176\nNeg Pred Value                0.9721            0.8106                  0.99692\nPrevalence                    0.1423            0.4728                  0.01554\nDetection Rate                0.1184            0.3664                  0.01255\nDetection Prevalence          0.1441            0.4381                  0.03048\nBalanced Accuracy             0.9010            0.8195                  0.89474\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                     0.53333        0.384615        0.7020\nSpecificity                     0.96929        0.980571        0.8686\nPos Pred Value                  0.32432        0.238095        0.7218\nNeg Pred Value                  0.98687        0.990190        0.8571\nPrevalence                      0.02690        0.015541        0.3270\nDetection Rate                  0.01435        0.005977        0.2295\nDetection Prevalence            0.04423        0.025105        0.3180\nBalanced Accuracy               0.75131        0.682593        0.7853\n\n:::"
  }
]